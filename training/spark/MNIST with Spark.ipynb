{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have each Python cell auto-formatted\n",
    "# See: https://black.readthedocs.io\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MNIST with Spark and Horovod\n",
    "\n",
    "## Introduction\n",
    "Recognizing handwritten digits based on the [MNIST (Modified National Institute of Standards and Technology) data set](http://yann.lecun.com/exdb/mnist/) is the \"Hello, World\" example of machine learning.\n",
    "Each (anti-aliased) black-and-white image represents a digit from 0 to 9 and has been fit into a 28x28 pixel bounding box.\n",
    "The problem of recognizing digits from handwriting is, for instance, important to the postal service when automatically reading zip codes from envelopes.\n",
    "\n",
    "### What You'll Learn\n",
    "We'll show you how to use Spark to build a simple Keras model to perform the multi-class classification of images provided.\n",
    "The example in the notebook includes both training a model in the notebook and running a distributed training job on the cluster using <a href=\"https://github.com/horovod/horovod\">Horovod</a>, so you can easily scale up your own models.\n",
    "[Horovod is best](https://github.com/horovod/horovod/blob/master/docs/spark.rst) when you want to build estimators with Keras or if you want to train on Spark `DataFrame`s from `pyspark`.\n",
    "Of course, you can also use Spark's [MLlib](https://spark.apache.org/docs/2.4.5/ml-guide.html) if you prefer.\n",
    "They key is that if you already have a Keras model, you may not want to rewrite it in Spark, but you may still want to leverage the power of distributed training with Spark and Horovod.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Horovod and Istio</b><br>\n",
    "    If Istio is enabled in the current namespace (ask your administrator), <a href=\"https://github.com/horovod/horovod/issues/1855\">Horovod on Spark does not work from within a notebook</a>.\n",
    "    To disable Istio, please use the configuration parameter <code>enableIstioInUserNamespaces</code> for any <em>newly created</em> namespaces.\n",
    "    This parameter must be specified at installation time.\n",
    "    Alternatively, you can disable it at the application level (see below).\n",
    "</div>\n",
    "\n",
    "For the distributed training job you'll need to package the complete trainer code in a Docker image.\n",
    "We'll show you how to do that with Kubeflow Fairing, so that you do not have to leave your favourite notebook environment at all!\n",
    "We'll also include instructions for local development, in case you prefer that.\n",
    "\n",
    "### What You'll Need\n",
    "All you need is this notebook.\n",
    "If you prefer to create your Docker image locally (i.e. outside of the Kubernetes cluster), you must have a [Docker](https://www.docker.com/products/container-runtime) client on your machine and configured to work with your own container registry.\n",
    "For Kubernetes commands to be run outside of the cluster, [`kubectl`](https://kubernetes.io/docs/reference/kubectl/kubectl/) is required.\n",
    "\n",
    "## Prerequisites\n",
    "All pre-built images include Spark and Horovod, so we're ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"mnist.py\"\n",
    "KUBERNETES_FILE = \"sparkapp-mnist.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
    "To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created (e.g. already exists)\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Cannot get resource as its creation failed: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model in the Notebook\n",
    "Since we ultimately want to train the model in a distributed fashion (potentially on GPUs), we put all the code in a single cell.\n",
    "That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "trainer_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import horovod.spark\n",
    "import horovod.tensorflow.keras as hvd\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def get_dataset(rank=0, size=1):\n",
    "    with np.load('datasets/mnist.npz', allow_pickle=True) as f:\n",
    "        x_train = f['x_train'][rank::size]\n",
    "        y_train = f['y_train'][rank::size]\n",
    "        x_test = f['x_test'][rank::size]\n",
    "        y_test = f['y_test'][rank::size]\n",
    "        x_train, x_test = x_train / 255.0, x_test / 255.0 # Normalize RGB values to [0, 1]\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def deserialize(model_bytes):\n",
    "    import horovod.tensorflow.keras as hvd\n",
    "    import h5py\n",
    "    import io\n",
    "    bio = io.BytesIO(model_bytes)\n",
    "    with h5py.File(bio, 'a') as f:\n",
    "        return hvd.load_model(f)\n",
    "\n",
    "\n",
    "def predict_number(model, x_test, image_index):\n",
    "    pred = model.predict(x_test[image_index:image_index + 1])\n",
    "    print(f\"Model prediction for index {image_index}: {pred.argmax()}\")\n",
    "\n",
    "\n",
    "def train_hvd(learning_rate, batch_size, epochs):\n",
    "    # 1 - Initialize Horovod\n",
    "    hvd.init()\n",
    "\n",
    "    # 2 - Pin GPUs\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "    model = get_model()\n",
    "\n",
    "    # 3 - Wrap optimizer\n",
    "    optimizer = hvd.DistributedOptimizer(\n",
    "        # 4- Scale learning rate\n",
    "        tf.optimizers.Adam(lr=learning_rate * hvd.size())\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  experimental_run_tf_function=False,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        # 5 - Broadcast initial variables\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "        hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=3, verbose=1),\n",
    "    ]\n",
    "\n",
    "    # 6 - Save checkpoints\n",
    "    ckpt_dir = tempfile.mkdtemp()\n",
    "    ckpt_file = os.path.join(ckpt_dir, 'checkpoint.h5')\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ModelCheckpoint(ckpt_file, monitor='accuracy', mode='max',\n",
    "                                               save_best_only=True))\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks,\n",
    "                        epochs=epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        with open(ckpt_file, 'rb') as f:\n",
    "            return history.history, f.read()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Horovod-on-Spark MNIST Training Job\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=int,\n",
    "        default=0.001,\n",
    "        metavar=\"N\",\n",
    "        help=\"Learning rate (default: 0.001)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"Batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"N\",\n",
    "        help=\"Number of epochs to train (default: 5)\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    spark = SparkSession.builder.appName(\"HorovodOnSpark\").getOrCreate()\n",
    "\n",
    "    image_index = 100\n",
    "    (x_train, y_train), (x_test, y_test) = get_dataset()\n",
    "    \n",
    "    print(f\"Expected prediction for index {image_index}: {y_test[image_index]}\")\n",
    "    \n",
    "    # Train model with Horovod on Spark\n",
    "    model_bytes = horovod.spark.run(train_hvd, args=(args.learning_rate,\n",
    "                                                     args.batch_size,\n",
    "                                                     args.epochs))[0][1]\n",
    "\n",
    "    model = deserialize(model_bytes)\n",
    "    model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "    predict_number(model, x_test, image_index)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [several things](https://github.com/horovod/horovod#concepts) worth highlighting:\n",
    "\n",
    "1. Horovod is initialized with `hvd.init()`.\n",
    "2. Each GPU must be pinned to a single process to avoid resource contention.\n",
    "3. The model's optimizer must be wrapped in `hvd.DistributedOptimizer`, which delegates gradient computations to the original optimizers (here: Adam) but applies averaged gradients.\n",
    "4. The learning rate must be scaled by the number of workers because the effective batch size is scaled by the number of workers, which is compensated by an increased learning rate.\n",
    "5. Initial variables must be broadcast from rank 0 to all other processes\n",
    "6. Checkpoints must only be created on worker 0 to prevent any corruption.\n",
    "\n",
    "Horovod is designed to scale on servers with multiple GPUs.\n",
    "\n",
    "Horovod relies on [MPI](https://github.com/horovod/horovod/blob/master/docs/concepts.rst) (Message Passing Interface) and therefore recycles some of its terminology:\n",
    "- The number of processes is called the **size**.\n",
    "- The unique process identifier, which runs from `0` to `size - 1`.\n",
    "- The local rank is the unique process identifier within each server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HOROVOD_JOB=mnist.py\n"
     ]
    }
   ],
   "source": [
    "%env HOROVOD_JOB=$TRAINER_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the training job, let's first run it on Spark in a local mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected prediction for index 100: 6\n",
      "Running 1 processes (inferred from spark.default.parallelism)...\n",
      "20/06/18 16:09:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "...\n",
      "[1,0]<stdout>:938/938 - 2s - loss: 0.3325 - accuracy: 0.9046 - val_loss: 0.1697 - val_accuracy: 0.9500 - lr: 0.0010\n",
      "313/313 - 0s - loss: 0.1697 - accuracy: 0.9500\n",
      "Model prediction for index 100: 6\n",
      "...\n",
      "20/06/18 16:09:30 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "! ${SPARK_HOME}/bin/spark-submit --master local[1] $HOROVOD_JOB --epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "trainer_dockerfile"
    ]
   },
   "source": [
    "This trains the model in the notebook, but does not distribute the procedure.\n",
    "To that end, we have to build-and-push a container image that contains the code and input dataset.\n",
    "\n",
    "We include the data set, so that the tutorial works on air-gapped (i.e. private/offline) clusters.\n",
    "MNIST data sets are typically downloaded on the fly, which would fail in such scenarios.\n",
    "In most realistic cases, the data sets would be available to the cluster as a volume.\n",
    "\n",
    "## How to Create a Docker Image with Kubeflow Fairing\n",
    "Kubeflow Fairing is a Python SDK that allows you to build, push, and optionally run containerized ML models without leaving Jupyter!\n",
    "To build and push Docker images from within a notebook, please check out the [Kubeflow Fairing notebook](../../fairing/Kubeflow%20Fairing.ipynb).\n",
    "All you need is the `TRAINER_FILE` and access to a container registry.\n",
    "\n",
    "## How to Create a Docker Image Manually\n",
    "If you are comfortable with Docker (or prefer to use it as a part of your CI/CD setup), you can create a Dockerfile as follows.\n",
    "You do have to download the `TRAINER_FILE` contents and the `datasets` directory to your local machine.\n",
    "The Kubernetes cluster does not have a Docker daemon available to build your image, so you must do it locally.\n",
    "It uses [containerd](https://containerd.io/) to run workloads (only) instead.\n",
    "\n",
    "The Dockerfile looks as follows:\n",
    "\n",
    "```\n",
    "FROM mesosphere/kubeflow:1.0.1-0.5.0-spark-3.0.0-horovod-0.19.5-tensorflow-2.2.0\n",
    "ADD mnist.py /\n",
    "ADD datasets /datasets\n",
    "\n",
    "WORKDIR /\n",
    "```\n",
    "\n",
    "If GPU support is not needed, you can leave off the `-gpu` suffix from the image.\n",
    "`mnist.py` is the trainer code you have to download to your local machine.\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "```bash\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "```\n",
    "\n",
    "The image is available as `mesosphere/kubeflow:mnist-spark-1.0.1-0.5.0` in case you want to skip it for now.\n",
    "\n",
    "## How to Create a Distributed `SparkApplication`\n",
    "The [KUDO Spark Operator](https://github.com/kudobuilder/operators/tree/master/repository/spark/docs/latest) manages Spark applications in a similar way as the [PyTorch](../pytorch/MNIST%20with%PyTorch.ipynb) or [TensorFlow](../tensorflow/MNIST%20with%20TensorFlow.ipynb) operators manage `PyTorchJob`s and `TFJob`s, respectively. \n",
    "It exposes a resource called `SparkApplication` that we shall use to train our model on multiple nodes with Horovod.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Kubernetes Nomenclature</b><br>\n",
    "    <code>SparkApplication</code> is a <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">custom resource (definition) (CRD)</a> provided by the KUDO <a href=\"https://github.com/mesosphere/kudo-spark-operator\">Spark operator</a>.\n",
    "    <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">Operators</a> extend Kubernetes by capturing domain-specific knowledge on how to deploy and run an application or service, how to deal with failures, and so on.\n",
    "    The lifecycle of a <code>SparkApplication</code> is managed by the Spark operator controller.\n",
    "    <a href=\"https://kudo.dev\">KUDO</a> is a toolkit for creating custom Kubernetes operators using YAML instead of writing Go code.\n",
    "</div>\n",
    "\n",
    "The specification for a distributed `SparkApplication` is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sparkapp-mnist.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: horovod-mnist\n",
    "spec:\n",
    "  type: Python\n",
    "  mode: cluster\n",
    "  pythonVersion: \"3\"\n",
    "  image: mesosphere/kubeflow:mnist-spark-1.0.1-0.5.0\n",
    "  imagePullPolicy: Always  \n",
    "  mainApplicationFile: \"local:///mnist.py\"\n",
    "  sparkVersion: \"3.0.0\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  arguments:\n",
    "    - --epochs\n",
    "    - \"10\"\n",
    "  driver:\n",
    "    env:\n",
    "    - name: PYTHONUNBUFFERED\n",
    "      value: \"1\"\n",
    "    cores: 1\n",
    "    memory: \"1G\"\n",
    "    labels:\n",
    "      version: 3.0.0\n",
    "      metrics-exposed: \"true\"  \n",
    "    annotations:\n",
    "      sidecar.istio.io/inject: \"false\"\n",
    "    serviceAccount: default-editor\n",
    "  executor:\n",
    "    cores: 1\n",
    "    instances: 5\n",
    "    memory: \"512m\"\n",
    "    labels:\n",
    "      version: 3.0.0\n",
    "      metrics-exposed: \"true\"  \n",
    "    annotations:\n",
    "      sidecar.istio.io/inject: \"false\"\n",
    "  monitoring:\n",
    "    exposeDriverMetrics: true\n",
    "    exposeExecutorMetrics: true\n",
    "    prometheus:\n",
    "      jmxExporterJar: \"/prometheus/jmx_prometheus_javaagent-0.11.0.jar\"\n",
    "      port: 8090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operator's user guide explains [how to configure the application](https://github.com/mesosphere/spark-on-k8s-operator/blob/master/docs/user-guide.md).\n",
    "\n",
    "Please note that in `spec.mainApplicationFile` the file name `/mnist.py` is hard coded and it must match the file in the docker image.\n",
    "\n",
    "The annotation `sidecar.istio.io/inject: \"false\"` disables Istio on the `SparkApplication` level.\n",
    "This is not needed if Istio is disabled at the namespace level by your administrator.\n",
    "\n",
    "You can either execute the following commands on your local machine with `kubectl` or directly from the notebook.\n",
    "If you do run these locally, you cannot rely on cell magic, so you have to manually copy-paste the variables' values wherever you see `$SOME_VARIABLE`.\n",
    "If you execute the following commands on your own machine (and not inside the notebook), you obviously do not need the bang `!` either.\n",
    "In that case, you have to set the user namespace for all subsequent commands:\n",
    "\n",
    "```\n",
    "kubectl config set-context --current --namespace=<insert-namespace>\n",
    "```\n",
    "\n",
    "Please change the namespace to whatever has been set up by your administrator.\n",
    "\n",
    "Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture hvd_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HVD_JOB = get_resource(hvd_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the pods are being created according to our specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS      RESTARTS   AGE\n",
      "horovod-mnist-driver   0/1     Completed   0          64s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l sparkoperator.k8s.io/app-name=horovod-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the model prediction (as before) by looking at the logs of the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction for index 100: 6\n"
     ]
    }
   ],
   "source": [
    "! kubectl logs horovod-mnist-driver | grep 'Model prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we can see the status of the `horovod-mnist` `SparkApplication`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         horovod-mnist\n",
      "...\n",
      "API Version:  sparkoperator.k8s.io/v1beta2\n",
      "Kind:         SparkApplication\n",
      "...\n",
      "Events:\n",
      "  Type    Reason                     Age   From            Message\n",
      "  ----    ------                     ----  ----            -------\n",
      "  Normal  SparkApplicationAdded      75s   spark-operator  SparkApplication horovod-mnist was added, enqueuing it for submission\n",
      "  Normal  SparkApplicationSubmitted  72s   spark-operator  SparkApplication horovod-mnist was submitted successfully\n",
      "  Normal  SparkDriverRunning         70s   spark-operator  Driver horovod-mnist-driver is running\n",
      "  Normal  SparkExecutorPending       65s   spark-operator  Executor horovod-mnist-1592496574728-exec-1 is pending\n",
      "  Normal  SparkExecutorPending       64s   spark-operator  Executor horovod-mnist-1592496574728-exec-2 is pending\n",
      "  Normal  SparkExecutorPending       64s   spark-operator  Executor horovod-mnist-1592496574728-exec-3 is pending\n",
      "  Normal  SparkExecutorPending       64s   spark-operator  Executor horovod-mnist-1592496574728-exec-4 is pending\n",
      "  Normal  SparkExecutorPending       64s   spark-operator  Executor horovod-mnist-1592496574728-exec-5 is pending\n",
      "  Normal  SparkExecutorRunning       63s   spark-operator  Executor horovod-mnist-1592496574728-exec-1 is running\n",
      "  Normal  SparkExecutorRunning       62s   spark-operator  Executor horovod-mnist-1592496574728-exec-3 is running\n",
      "  Normal  SparkExecutorRunning       62s   spark-operator  Executor horovod-mnist-1592496574728-exec-2 is running\n",
      "  Normal  SparkExecutorRunning       61s   spark-operator  Executor horovod-mnist-1592496574728-exec-4 is running\n",
      "  Normal  SparkExecutorRunning       61s   spark-operator  Executor horovod-mnist-1592496574728-exec-5 is running\n",
      "  Normal  SparkDriverCompleted       23s   spark-operator  Driver horovod-mnist-driver completed\n",
      "  Normal  SparkApplicationCompleted  23s   spark-operator  SparkApplication horovod-mnist completed\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe $HVD_JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkapplication.sparkoperator.k8s.io \"horovod-mnist\" deleted\n"
     ]
    }
   ],
   "source": [
    "! kubectl delete $HVD_JOB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
