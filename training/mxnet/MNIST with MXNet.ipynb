{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have each Python cell auto-formatted\n",
    "# See: https://black.readthedocs.io\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MNIST with MXNet\n",
    "\n",
    "## Introduction\n",
    "Recognizing handwritten digits based on the [MNIST (Modified National Institute of Standards and Technology) data set](http://yann.lecun.com/exdb/mnist/) is the \"Hello, World\" example of machine learning.\n",
    "Each (anti-aliased) black-and-white image represents a digit from 0 to 9 and has been fit into a 28x28 pixel bounding box.\n",
    "The problem of recognizing digits from handwriting is, for instance, important to the postal service when automatically reading zip codes from envelopes.\n",
    "\n",
    "### What You'll Learn\n",
    "We'll show you how to use Apache MXNet to build a model with two convolutional layers and two fully connected layers to perform the multi-class classification of images provided.\n",
    "An approach using the `Sequential` layer type is available [here](https://mxnet.incubator.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html).\n",
    "\n",
    "### What You'll Need\n",
    "All you need is this notebook.\n",
    "\n",
    "## How to Load and Inspect the Data\n",
    "Before we proceed, let's check that we're using the right image, that is, [MXNet](https://mxnet.apache.org/api/python/docs/api/) is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxnet-cu102mkl           1.6.0"
     ]
    }
   ],
   "source": [
    "! pip list | grep mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!\n",
    "\n",
    "Let's import the necessary Python modules and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "import gzip\n",
    "import logging\n",
    "import struct\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def get_mnist():\n",
    "    \"\"\"\n",
    "    Utility method to load the MNIST dataset stored on disk.\n",
    "    This is a modification of the original test_utils.get_mnist() function available in MXNet.\n",
    "    \n",
    "    Link:\n",
    "    https://mxnet.apache.org/versions/1.6/api/python/docs/_modules/mxnet/test_utils.html#get_mnist\n",
    "    \"\"\"\n",
    "    def read_data(label_url, image_url):\n",
    "        with gzip.open(label_url) as flbl:\n",
    "            struct.unpack(\">II\", flbl.read(8))\n",
    "            label = np.frombuffer(flbl.read(), dtype=np.int8)\n",
    "        with gzip.open(image_url, \"rb\") as fimg:\n",
    "            _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "            image = np.frombuffer(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "            image = image.reshape(image.shape[0], 1, 28, 28).astype(np.float32)/255\n",
    "        return label, image\n",
    "\n",
    "    path = \"datasets/mnist/\"\n",
    "    (train_lbl, train_img) = read_data(path+'train-labels-idx1-ubyte.gz', path+'train-images-idx3-ubyte.gz')\n",
    "    (test_lbl, test_img) = read_data(path+'t10k-labels-idx1-ubyte.gz', path+'t10k-images-idx3-ubyte.gz')\n",
    "    \n",
    "    return {'train_data':train_img, 'train_label':train_lbl, 'test_data':test_img, 'test_label':test_lbl}\n",
    "\n",
    "\n",
    "mnist = get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to receive a `dict`, so for future reference we can look at the available keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_data', 'train_label', 'test_data', 'test_label'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the data structured?\n",
    "We can see that by grabbing an example and asking for the shape of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = mnist[\"train_data\"][42]\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have `(batch, height, width)` because `batch = 1` for a single example.\n",
    "For RGB images, we'd have `(batch, height, width, channels)` with `channels = 3`.\n",
    "What does the image itself look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(np.squeeze(example))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That could be both a 1 or a 7, so let's check what the label says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"train_label\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be on the safe side, we need to check our pixel values have already been scaled into the [0, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = example.flatten()\n",
    "min(flattened), max(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model\n",
    "\n",
    "We shall make use of the following convenience function to create a single convolutional layer with a certain `activation` function followed by a pre-defined max pooling layer.\n",
    "Since we intend to have two such layers in our model, it makes sense to package a single layer as a re-usable function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>A Note on Activation Functions</b><br>\n",
    "    A common choice for <a href=\"https://arxiv.org/abs/1606.02228\">activation functions</a> is a ReLU (Rectified Linear Unit).\n",
    "    It is linear for non-negative values and zero for negative ones.\n",
    "    The <a href=\"https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\">main benefits of ReLU</a> as opposed to sigmoidal functions (e.g. logistic or `tanh`) are:\n",
    "    <ul>\n",
    "        <li>ReLU and its gradient are very cheap to compute;</li>\n",
    "        <li>Gradients are less likely to vanish, because for (non-)negative values its gradient is constant and therefore does not saturate, which for deep neural networks can <a href=\"https://dl.acm.org/doi/10.1145/3065386\">accelerate convergence</a></li>\n",
    "        <li>ReLU has a regularizing effect, because it promotes <a href=\"https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks\">sparse representations</a> (i.e. some nodes' weights are zero);</li> \n",
    "        <li>Empirically it has been found to work well.</li>\n",
    "    </ul>\n",
    "    ReLU activation functions can cause neurons to 'die' because a large, negative (learned) bias value causes all inputs to be negative, which in turn leads to a zero output.\n",
    "    The neuron has thus become incapable of discriminating different input values.\n",
    "    So-called leaky ReLU activations functions address that issue; these functions are linear but non-zero for negative values, so that their gradients are small but non-zero.\n",
    "    <a href=\"https://arxiv.org/abs/1511.07289\">ELUs</a>, or exponential linear units, are another solution to the problem of dying neurons.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_layer, kernel, num_filters, activation):\n",
    "    \"\"\"\n",
    "    Defines a CNN layer with `activation` function and 2D max pooling with a kernel and stride of (2, 2)\n",
    "    \n",
    "    :param layer: input layer (an MXNet symbol)\n",
    "    :param kernel: 2D convolutional kernel\n",
    "    :param filters: number of filters to use in convolution\n",
    "    :param activation: activation function (e.g. \"tanh\" or \"relu\")\n",
    "    :rtype: mxnet.symbol.symbol.Symbol\n",
    "    \"\"\"\n",
    "    conv = mx.sym.Convolution(data=input_layer, kernel=kernel, num_filter=num_filters)\n",
    "    act = mx.sym.Activation(data=conv, act_type=activation)\n",
    "    pool = mx.sym.Pooling(data=act, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>A Note on CNNs</b><br>\n",
    "    While it is not our intention to cover the basics of <a href=\"https://www.deeplearningbook.org/contents/convnets.html\">convolutional neural networks</a> (CNNs), there are a few matters worth mentioning.\n",
    "    Convolutional layers are spatial feature extractors for images.\n",
    "    A series of convolutional kernels (of the same dimensions) is applied to the image to obtain different versions of the same base image (i.e. filters).\n",
    "    These filters extract patterns hierarchically.\n",
    "    In the first layer, filters typically capture dots, edges, corners, and so on.\n",
    "    With each additional layer, these patterns become more complex and turn from basic geometric shapes into constituents of objects and entire objects.\n",
    "    That is why often the number of filters increases with each additional convolutional layer: to extract more complex patterns.<br><br>\n",
    "    Convolutional layers are often followed by a pooling layer to down-sample the input.\n",
    "    This aids in lowering the computational burden as we increase the number of filters.\n",
    "    A max pooling layer simply picks the largest value of pixels in a small (rectangular) neighbourhood of a single channel (e.g. RGB). \n",
    "    This has the effect of making features <em>locally</em> translation-invariant, which is often desired: whether a feature of interest is on the left or right edge of a pooling window, which is also referred to as a kernel, is largely irrelevant to the problem of image classification.\n",
    "    Note that this may not always be a desired characteristic and depends on the size of the pooling kernel.\n",
    "    For instance, the precice location of tissue damage in living organisms or defects on manufactured products may be very significant indeed.\n",
    "    Pooling kernels are generally chosen to be relatively small compared to the dimensions of the input, which means that local translation invariance is often desired. <br><br>\n",
    "    Another common component of CNNs is a dropout layer.\n",
    "    <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout</a> provides a mechanism for regularization that has proven successful in many applications.\n",
    "    It is suprisingly simple: some nodes' weights (and biases) in a specific layer are set to zero <em>at random</em>, that is, arbitrary nodes are removed from the network during the training step.\n",
    "    This causes the network to not rely on any single node (a.k.a. neuron) for a feature, as each node can be dropped at random.\n",
    "    The network therefore has to learn redundant representations of features.\n",
    "    This is important because of what is referred to as <em>internal covariate shift</em> (often mentioned in connection with <a href=\"http://proceedings.mlr.press/v37/ioffe15.html\">batch normalization</a>): the change of distributions of internal nodes' weights due to all other layers, which can cause nodes to stop learning (i.e. updating their weights).\n",
    "    Thanks to dropout, layers become more robust to changes, although it also means it limits what can be learnt (as always with regularization).\n",
    "    Still, dropout is the neural network's equivalent of the saying you should never put all your eggs in one basket.\n",
    "    Layers with a high risk of overfitting (e.g. layers with many units and lots of inputs) typically have a higher dropout rate.\n",
    "    <br><br>\n",
    "    A nice visual explanation of convolutional layers is available <a href=\"https://cezannec.github.io/Convolutional_Neural_Networks/\">here</a>.\n",
    "    If you're curious what a CNN \"sees\" while training, you can have a look <a href=\"https://poloclub.github.io/cnn-explainer/\">here</a>.\n",
    "</div>\n",
    "\n",
    "With the following function we create an ANN with two convolutional layers (as defined above), two fully connected layers with a different number of , and an output layer with a softmax function.\n",
    "In each of the layers, we choose the same activation function, although that is not needed and can easily be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(input_layer, kernels, filters, activation, hidden_units):\n",
    "    \"\"\"\n",
    "    Defines a neural network with two convolutional layers and two dense layers.\n",
    "    To train the model it needs to be wrapped in a module.\n",
    "    \n",
    "    :param input_layer: input layer (an MXNet symbol)\n",
    "    :param kernels: a list of 2D convolutional kernels\n",
    "    :param filters: a list of convolutional filters\n",
    "    :param activation: the activation function for all layers (e.g. \"tanh\" or \"relu\")\n",
    "    :param hidden_units: a list of hidden units for the dense layers\n",
    "    :rtype: mxnet.symbol.symbol.Symbol\n",
    "    \"\"\"\n",
    "    conv1 = conv_layer(\n",
    "        input_layer=input_layer,\n",
    "        kernel=kernels[0],\n",
    "        num_filters=filters[0],\n",
    "        activation=activation,\n",
    "    )\n",
    "    conv2 = conv_layer(\n",
    "        input_layer=conv1,\n",
    "        kernel=kernels[1],\n",
    "        num_filters=filters[1],\n",
    "        activation=activation,\n",
    "    )\n",
    "\n",
    "    flattened = mx.sym.flatten(data=conv2)\n",
    "\n",
    "    fc1 = mx.sym.FullyConnected(data=flattened, num_hidden=hidden_units[0])\n",
    "    fc1_out = mx.sym.Activation(data=fc1, act_type=activation)\n",
    "\n",
    "    fc2 = mx.sym.FullyConnected(data=fc1_out, num_hidden=hidden_units[1])\n",
    "\n",
    "    out = mx.sym.SoftmaxOutput(data=fc2, name=\"softmax\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define an execution context for the model training.\n",
    "If GPUs are available the model is trained on there.\n",
    "If not, it defaults to using CPUs.\n",
    "\n",
    "Since we can use the context across training instances (e.g. if we want to see the effect of a different activation function), we can define it outside the main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()\n",
    "if mx.context.num_gpus() > 0:\n",
    "    context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data,\n",
    "    context,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate=0.1,\n",
    "    activation=\"tanh\",\n",
    "    kernels=[(5, 5), (5, 5)],\n",
    "    filters=[20, 50],\n",
    "    hidden_units=[500, 10],\n",
    "):\n",
    "    # Create an iterator for the training data with a fixed `batch_size`\n",
    "    # We shuffle the data to ensure each batch is representative of the entire data set\n",
    "    # Note that we can also use `mxnet.test_utils.get_mnist_iterator`\n",
    "    train_iter = mx.io.NDArrayIter(\n",
    "        data[\"train_data\"], data[\"train_label\"], batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Create an iterator for the test data with a fixed `batch_size`\n",
    "    # No need to shuffle the test data!\n",
    "    eval_iter = mx.io.NDArrayIter(data[\"test_data\"], data[\"test_label\"], batch_size)\n",
    "\n",
    "    # Define a symbolic (placeholder) variable\n",
    "    images = mx.sym.Variable(\"data\")\n",
    "\n",
    "    # Create the artificial neural network based\n",
    "    net = ann(images, kernels, filters, activation, hidden_units)\n",
    "\n",
    "    # To be able to train (and evaluate) a model, we need the execution `context`,\n",
    "    # and we must wrap the (output) symbol `net` in a module.\n",
    "    model = mx.module.Module(symbol=net, context=context)\n",
    "\n",
    "    # Train using a stochastic gradient descent algorithm with respect to the test accuracy\n",
    "    model.fit(\n",
    "        train_iter,\n",
    "        eval_data=eval_iter,\n",
    "        optimizer=\"sgd\",\n",
    "        optimizer_params={\"learning_rate\": learning_rate},\n",
    "        eval_metric=\"acc\",\n",
    "        batch_end_callback=mx.callback.Speedometer(batch_size, 100),\n",
    "        num_epoch=epochs,\n",
    "    )\n",
    "\n",
    "    # Define another iterator and use the `score` method our `model` module\n",
    "    # to compute the accuracy.\n",
    "    # Note: We can see the accuracy on the training and test data set from the logs,\n",
    "    # but it's convenient to return it from the function, together with the model, so\n",
    "    # we can call `model.predict(...)` on it.\n",
    "    test_iter = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size)\n",
    "    acc = mx.metric.Accuracy()\n",
    "    model.score(test_iter, acc)\n",
    "    return model, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke it with the defaults (and our pre-defined parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [0-100]\tSpeed: 1353.96 samples/sec\taccuracy=0.114356\n",
      "INFO:root:Epoch[0] Batch [100-200]\tSpeed: 1438.84 samples/sec\taccuracy=0.113700\n",
      "INFO:root:Epoch[0] Batch [200-300]\tSpeed: 1464.64 samples/sec\taccuracy=0.112600\n",
      "INFO:root:Epoch[0] Batch [300-400]\tSpeed: 1523.29 samples/sec\taccuracy=0.109100\n",
      "INFO:root:Epoch[0] Batch [400-500]\tSpeed: 1467.58 samples/sec\taccuracy=0.113200\n",
      "INFO:root:Epoch[0] Train-accuracy=0.112033\n",
      "INFO:root:Epoch[0] Time cost=41.259\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.113500\n",
      "INFO:root:Epoch[1] Batch [0-100]\tSpeed: 1490.50 samples/sec\taccuracy=0.113069\n",
      "INFO:root:Epoch[1] Batch [100-200]\tSpeed: 1425.70 samples/sec\taccuracy=0.202600\n",
      "INFO:root:Epoch[1] Batch [200-300]\tSpeed: 1297.02 samples/sec\taccuracy=0.698700\n",
      "INFO:root:Epoch[1] Batch [300-400]\tSpeed: 1369.69 samples/sec\taccuracy=0.866400\n",
      "INFO:root:Epoch[1] Batch [400-500]\tSpeed: 1276.43 samples/sec\taccuracy=0.904900\n",
      "INFO:root:Epoch[1] Train-accuracy=0.616617\n",
      "INFO:root:Epoch[1] Time cost=44.176\n",
      "INFO:root:Epoch[1] Validation-accuracy=0.931500\n",
      "INFO:root:Epoch[2] Batch [0-100]\tSpeed: 1347.37 samples/sec\taccuracy=0.935446\n",
      "INFO:root:Epoch[2] Batch [100-200]\tSpeed: 1332.14 samples/sec\taccuracy=0.949500\n",
      "INFO:root:Epoch[2] Batch [200-300]\tSpeed: 1287.23 samples/sec\taccuracy=0.955100\n",
      "INFO:root:Epoch[2] Batch [300-400]\tSpeed: 1245.89 samples/sec\taccuracy=0.956200\n",
      "INFO:root:Epoch[2] Batch [400-500]\tSpeed: 1289.72 samples/sec\taccuracy=0.961800\n",
      "INFO:root:Epoch[2] Train-accuracy=0.953717\n",
      "INFO:root:Epoch[2] Time cost=46.817\n",
      "INFO:root:Epoch[2] Validation-accuracy=0.967500\n",
      "INFO:root:Epoch[3] Batch [0-100]\tSpeed: 1187.91 samples/sec\taccuracy=0.969604\n",
      "INFO:root:Epoch[3] Batch [100-200]\tSpeed: 1267.84 samples/sec\taccuracy=0.968600\n",
      "INFO:root:Epoch[3] Batch [200-300]\tSpeed: 1298.08 samples/sec\taccuracy=0.971600\n",
      "INFO:root:Epoch[3] Batch [300-400]\tSpeed: 1300.17 samples/sec\taccuracy=0.976300\n",
      "INFO:root:Epoch[3] Batch [400-500]\tSpeed: 1224.72 samples/sec\taccuracy=0.974500\n",
      "INFO:root:Epoch[3] Train-accuracy=0.973083\n",
      "INFO:root:Epoch[3] Time cost=47.699\n",
      "INFO:root:Epoch[3] Validation-accuracy=0.977000\n",
      "INFO:root:Epoch[4] Batch [0-100]\tSpeed: 1233.89 samples/sec\taccuracy=0.975842\n",
      "INFO:root:Epoch[4] Batch [100-200]\tSpeed: 1286.16 samples/sec\taccuracy=0.981800\n",
      "INFO:root:Epoch[4] Batch [200-300]\tSpeed: 1167.10 samples/sec\taccuracy=0.979000\n",
      "INFO:root:Epoch[4] Batch [300-400]\tSpeed: 1246.25 samples/sec\taccuracy=0.979700\n",
      "INFO:root:Epoch[4] Batch [400-500]\tSpeed: 1202.52 samples/sec\taccuracy=0.982900\n",
      "INFO:root:Epoch[4] Train-accuracy=0.980350\n",
      "INFO:root:Epoch[4] Time cost=49.019\n",
      "INFO:root:Epoch[4] Validation-accuracy=0.984200\n",
      "INFO:root:Epoch[5] Batch [0-100]\tSpeed: 1159.72 samples/sec\taccuracy=0.983366\n",
      "INFO:root:Epoch[5] Batch [100-200]\tSpeed: 1217.18 samples/sec\taccuracy=0.980900\n",
      "INFO:root:Epoch[5] Batch [200-300]\tSpeed: 1223.35 samples/sec\taccuracy=0.984400\n",
      "INFO:root:Epoch[5] Batch [300-400]\tSpeed: 1251.70 samples/sec\taccuracy=0.985000\n",
      "INFO:root:Epoch[5] Batch [400-500]\tSpeed: 1270.82 samples/sec\taccuracy=0.985400\n",
      "INFO:root:Epoch[5] Train-accuracy=0.984067\n",
      "INFO:root:Epoch[5] Time cost=48.736\n",
      "INFO:root:Epoch[5] Validation-accuracy=0.984500\n",
      "INFO:root:Epoch[6] Batch [0-100]\tSpeed: 1225.29 samples/sec\taccuracy=0.986139\n",
      "INFO:root:Epoch[6] Batch [100-200]\tSpeed: 1180.46 samples/sec\taccuracy=0.986300\n",
      "INFO:root:Epoch[6] Batch [200-300]\tSpeed: 1272.58 samples/sec\taccuracy=0.986600\n",
      "INFO:root:Epoch[6] Batch [300-400]\tSpeed: 1276.39 samples/sec\taccuracy=0.985300\n",
      "INFO:root:Epoch[6] Batch [400-500]\tSpeed: 1295.95 samples/sec\taccuracy=0.989000\n",
      "INFO:root:Epoch[6] Train-accuracy=0.986550\n",
      "INFO:root:Epoch[6] Time cost=48.073\n",
      "INFO:root:Epoch[6] Validation-accuracy=0.987700\n",
      "INFO:root:Epoch[7] Batch [0-100]\tSpeed: 1213.19 samples/sec\taccuracy=0.986931\n",
      "INFO:root:Epoch[7] Batch [100-200]\tSpeed: 1157.00 samples/sec\taccuracy=0.987300\n",
      "INFO:root:Epoch[7] Batch [200-300]\tSpeed: 1182.32 samples/sec\taccuracy=0.988800\n",
      "INFO:root:Epoch[7] Batch [300-400]\tSpeed: 1250.13 samples/sec\taccuracy=0.989800\n",
      "INFO:root:Epoch[7] Batch [400-500]\tSpeed: 1226.76 samples/sec\taccuracy=0.989100\n",
      "INFO:root:Epoch[7] Train-accuracy=0.988350\n",
      "INFO:root:Epoch[7] Time cost=49.748\n",
      "INFO:root:Epoch[7] Validation-accuracy=0.987400\n",
      "INFO:root:Epoch[8] Batch [0-100]\tSpeed: 1262.47 samples/sec\taccuracy=0.990990\n",
      "INFO:root:Epoch[8] Batch [100-200]\tSpeed: 1199.14 samples/sec\taccuracy=0.989800\n",
      "INFO:root:Epoch[8] Batch [200-300]\tSpeed: 1193.96 samples/sec\taccuracy=0.990500\n",
      "INFO:root:Epoch[8] Batch [300-400]\tSpeed: 1087.18 samples/sec\taccuracy=0.988200\n",
      "INFO:root:Epoch[8] Batch [400-500]\tSpeed: 1154.58 samples/sec\taccuracy=0.989800\n",
      "INFO:root:Epoch[8] Train-accuracy=0.989850\n",
      "INFO:root:Epoch[8] Time cost=50.834\n",
      "INFO:root:Epoch[8] Validation-accuracy=0.988700\n",
      "INFO:root:Epoch[9] Batch [0-100]\tSpeed: 1179.14 samples/sec\taccuracy=0.990297\n",
      "INFO:root:Epoch[9] Batch [100-200]\tSpeed: 1226.71 samples/sec\taccuracy=0.990800\n",
      "INFO:root:Epoch[9] Batch [200-300]\tSpeed: 1208.98 samples/sec\taccuracy=0.989800\n",
      "INFO:root:Epoch[9] Batch [300-400]\tSpeed: 1272.81 samples/sec\taccuracy=0.991500\n",
      "INFO:root:Epoch[9] Batch [400-500]\tSpeed: 1292.01 samples/sec\taccuracy=0.989900\n",
      "INFO:root:Epoch[9] Train-accuracy=0.990800\n",
      "INFO:root:Epoch[9] Time cost=48.257\n",
      "INFO:root:Epoch[9] Validation-accuracy=0.987700\n"
     ]
    }
   ],
   "source": [
    "model, acc = train(mnist, context, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>A Note on Accuracy</b><br>\n",
    "    We can see from the logs that the accuracy on both training and test data are relatively close.\n",
    "    A training accuracy that is significantly higher than the test accuracy is an indication of a model that overfits: it picks up on noise rather than the signal that's present in the data.\n",
    "    This model, therefore, does a decent job of classifying digits in images.\n",
    "</div>\n",
    "\n",
    "Because we wrapped out training process in a function, we can easily see the impact of a different activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [0-100]\tSpeed: 1693.27 samples/sec\taccuracy=0.111089\n",
      "INFO:root:Epoch[0] Batch [100-200]\tSpeed: 1739.83 samples/sec\taccuracy=0.118500\n",
      "INFO:root:Epoch[0] Batch [200-300]\tSpeed: 1768.59 samples/sec\taccuracy=0.105400\n",
      "INFO:root:Epoch[0] Batch [300-400]\tSpeed: 1870.50 samples/sec\taccuracy=0.112700\n",
      "INFO:root:Epoch[0] Batch [400-500]\tSpeed: 1777.87 samples/sec\taccuracy=0.111900\n",
      "INFO:root:Epoch[0] Train-accuracy=0.112017\n",
      "INFO:root:Epoch[0] Time cost=33.978\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.113500\n",
      "INFO:root:Epoch[1] Batch [0-100]\tSpeed: 1813.15 samples/sec\taccuracy=0.117228\n",
      "INFO:root:Epoch[1] Batch [100-200]\tSpeed: 1764.89 samples/sec\taccuracy=0.107800\n",
      "INFO:root:Epoch[1] Batch [200-300]\tSpeed: 1798.58 samples/sec\taccuracy=0.113700\n",
      "INFO:root:Epoch[1] Batch [300-400]\tSpeed: 1880.55 samples/sec\taccuracy=0.235100\n",
      "INFO:root:Epoch[1] Batch [400-500]\tSpeed: 1829.45 samples/sec\taccuracy=0.713800\n",
      "INFO:root:Epoch[1] Train-accuracy=0.360967\n",
      "INFO:root:Epoch[1] Time cost=32.823\n",
      "INFO:root:Epoch[1] Validation-accuracy=0.935800\n",
      "INFO:root:Epoch[2] Batch [0-100]\tSpeed: 1876.07 samples/sec\taccuracy=0.928416\n",
      "INFO:root:Epoch[2] Batch [100-200]\tSpeed: 1837.19 samples/sec\taccuracy=0.942200\n",
      "INFO:root:Epoch[2] Batch [200-300]\tSpeed: 1892.24 samples/sec\taccuracy=0.952900\n",
      "INFO:root:Epoch[2] Batch [300-400]\tSpeed: 1895.88 samples/sec\taccuracy=0.961800\n",
      "INFO:root:Epoch[2] Batch [400-500]\tSpeed: 1943.46 samples/sec\taccuracy=0.962000\n",
      "INFO:root:Epoch[2] Train-accuracy=0.952467\n",
      "INFO:root:Epoch[2] Time cost=31.772\n",
      "INFO:root:Epoch[2] Validation-accuracy=0.970900\n",
      "INFO:root:Epoch[3] Batch [0-100]\tSpeed: 1884.76 samples/sec\taccuracy=0.965248\n",
      "INFO:root:Epoch[3] Batch [100-200]\tSpeed: 1930.55 samples/sec\taccuracy=0.974600\n",
      "INFO:root:Epoch[3] Batch [200-300]\tSpeed: 1932.68 samples/sec\taccuracy=0.973000\n",
      "INFO:root:Epoch[3] Batch [300-400]\tSpeed: 1919.08 samples/sec\taccuracy=0.975500\n",
      "INFO:root:Epoch[3] Batch [400-500]\tSpeed: 1875.23 samples/sec\taccuracy=0.977300\n",
      "INFO:root:Epoch[3] Train-accuracy=0.973900\n",
      "INFO:root:Epoch[3] Time cost=31.617\n",
      "INFO:root:Epoch[3] Validation-accuracy=0.981600\n",
      "INFO:root:Epoch[4] Batch [0-100]\tSpeed: 1882.73 samples/sec\taccuracy=0.976832\n",
      "INFO:root:Epoch[4] Batch [100-200]\tSpeed: 1879.33 samples/sec\taccuracy=0.978900\n",
      "INFO:root:Epoch[4] Batch [200-300]\tSpeed: 1696.34 samples/sec\taccuracy=0.981700\n",
      "INFO:root:Epoch[4] Batch [300-400]\tSpeed: 1771.81 samples/sec\taccuracy=0.982200\n",
      "INFO:root:Epoch[4] Batch [400-500]\tSpeed: 1779.80 samples/sec\taccuracy=0.982500\n",
      "INFO:root:Epoch[4] Train-accuracy=0.980533\n",
      "INFO:root:Epoch[4] Time cost=33.584\n",
      "INFO:root:Epoch[4] Validation-accuracy=0.983500\n",
      "INFO:root:Epoch[5] Batch [0-100]\tSpeed: 1888.09 samples/sec\taccuracy=0.984653\n",
      "INFO:root:Epoch[5] Batch [100-200]\tSpeed: 1830.69 samples/sec\taccuracy=0.987400\n",
      "INFO:root:Epoch[5] Batch [200-300]\tSpeed: 1752.42 samples/sec\taccuracy=0.985500\n",
      "INFO:root:Epoch[5] Batch [300-400]\tSpeed: 1716.19 samples/sec\taccuracy=0.983000\n",
      "INFO:root:Epoch[5] Batch [400-500]\tSpeed: 1728.71 samples/sec\taccuracy=0.985200\n",
      "INFO:root:Epoch[5] Train-accuracy=0.985150\n",
      "INFO:root:Epoch[5] Time cost=33.577\n",
      "INFO:root:Epoch[5] Validation-accuracy=0.983600\n",
      "INFO:root:Epoch[6] Batch [0-100]\tSpeed: 1829.37 samples/sec\taccuracy=0.988119\n",
      "INFO:root:Epoch[6] Batch [100-200]\tSpeed: 1879.84 samples/sec\taccuracy=0.988000\n",
      "INFO:root:Epoch[6] Batch [200-300]\tSpeed: 1866.12 samples/sec\taccuracy=0.988600\n",
      "INFO:root:Epoch[6] Batch [300-400]\tSpeed: 1901.34 samples/sec\taccuracy=0.988500\n",
      "INFO:root:Epoch[6] Batch [400-500]\tSpeed: 1850.14 samples/sec\taccuracy=0.986400\n",
      "INFO:root:Epoch[6] Train-accuracy=0.988083\n",
      "INFO:root:Epoch[6] Time cost=32.148\n",
      "INFO:root:Epoch[6] Validation-accuracy=0.985400\n",
      "INFO:root:Epoch[7] Batch [0-100]\tSpeed: 1834.51 samples/sec\taccuracy=0.990891\n",
      "INFO:root:Epoch[7] Batch [100-200]\tSpeed: 1873.71 samples/sec\taccuracy=0.989500\n",
      "INFO:root:Epoch[7] Batch [200-300]\tSpeed: 1878.99 samples/sec\taccuracy=0.989000\n",
      "INFO:root:Epoch[7] Batch [300-400]\tSpeed: 1879.32 samples/sec\taccuracy=0.989200\n",
      "INFO:root:Epoch[7] Batch [400-500]\tSpeed: 1815.73 samples/sec\taccuracy=0.989600\n",
      "INFO:root:Epoch[7] Train-accuracy=0.989600\n",
      "INFO:root:Epoch[7] Time cost=32.270\n",
      "INFO:root:Epoch[7] Validation-accuracy=0.987400\n",
      "INFO:root:Epoch[8] Batch [0-100]\tSpeed: 1732.95 samples/sec\taccuracy=0.991188\n",
      "INFO:root:Epoch[8] Batch [100-200]\tSpeed: 1635.98 samples/sec\taccuracy=0.988900\n",
      "INFO:root:Epoch[8] Batch [200-300]\tSpeed: 1799.76 samples/sec\taccuracy=0.992300\n",
      "INFO:root:Epoch[8] Batch [300-400]\tSpeed: 1736.55 samples/sec\taccuracy=0.992600\n",
      "INFO:root:Epoch[8] Batch [400-500]\tSpeed: 1825.21 samples/sec\taccuracy=0.990400\n",
      "INFO:root:Epoch[8] Train-accuracy=0.991017\n",
      "INFO:root:Epoch[8] Time cost=34.618\n",
      "INFO:root:Epoch[8] Validation-accuracy=0.987800\n",
      "INFO:root:Epoch[9] Batch [0-100]\tSpeed: 1749.84 samples/sec\taccuracy=0.991881\n",
      "INFO:root:Epoch[9] Batch [100-200]\tSpeed: 1813.36 samples/sec\taccuracy=0.991900\n",
      "INFO:root:Epoch[9] Batch [200-300]\tSpeed: 1832.93 samples/sec\taccuracy=0.992500\n",
      "INFO:root:Epoch[9] Batch [300-400]\tSpeed: 1561.59 samples/sec\taccuracy=0.994700\n",
      "INFO:root:Epoch[9] Batch [400-500]\tSpeed: 1848.26 samples/sec\taccuracy=0.992400\n",
      "INFO:root:Epoch[9] Train-accuracy=0.992583\n",
      "INFO:root:Epoch[9] Time cost=34.517\n",
      "INFO:root:Epoch[9] Validation-accuracy=0.987800\n"
     ]
    }
   ],
   "source": [
    "model_relu, acc_relu = train(mnist, context, epochs, batch_size, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: EvalMetric: {'accuracy': 0.9877} (tanh) vs EvalMetric: {'accuracy': 0.9878} (ReLU)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc} (tanh) vs {acc_relu} (ReLU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a simple example of fiddling with hyperparameters.\n",
    "If we wanted to [tune hyperparameters (with Katib)](../../katib/Hyperparameter%20Tuning.ipynb) automatically, we could simply pass these hyperparameters as arguments to container that contains a script with all necessary imports and functions to run the train-and-evaluate process.\n",
    "\n",
    "## How to Predict with a Trained Model\n",
    "Batch predictions based on a trained model are easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iter(data=mnist, batch_size=100):\n",
    "    return mx.io.NDArrayIter(data[\"test_data\"], None, batch_size=batch_size)\n",
    "\n",
    "\n",
    "prob = model.predict(test_iter())\n",
    "prob_relu = model_relu.predict(test_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pick a random example, we can see the probabilities per category (i.e. digit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [3.88936355e-10 5.50869439e-09 1.38218335e-08 1.33717464e-11\n",
       "  9.99999046e-01 1.08974885e-09 3.25030669e-07 8.28973228e-08\n",
       "  1.59384072e-07 3.14834097e-07]\n",
       " <NDArray 10 @cpu(0)>,\n",
       " \n",
       " [6.4347176e-08 7.3004806e-08 3.4015596e-08 5.8196594e-09 9.9998724e-01\n",
       "  1.2476704e-07 1.9580840e-07 3.0599865e-06 1.2806310e-08 9.1640350e-06]\n",
       " <NDArray 10 @cpu(0)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[24], prob_relu[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest probabily is observed for the fourth index (i.e. the digit '4'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [4.]\n",
       " <NDArray 1 @cpu(0)>,\n",
       " \n",
       " [4.]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prob[24]), np.argmax(prob_relu[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we did not shuffle data for the iterator `test_iter` that was used to generate probabilities we can use the same index to obtain the label to verify that the model predicts the digit correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"test_label\"][24]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
