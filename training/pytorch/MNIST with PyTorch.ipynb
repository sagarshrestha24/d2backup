{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have each Python cell auto-formatted\n",
    "# See: https://black.readthedocs.io\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MNIST with PyTorch\n",
    "\n",
    "## Introduction\n",
    "Recognizing handwritten digits based on the [MNIST (Modified National Institute of Standards and Technology) data set](http://yann.lecun.com/exdb/mnist/) is the \"Hello, World\" example of machine learning.\n",
    "Each (anti-aliased) black-and-white image represents a digit from 0 to 9 and has been fit into a 28x28 pixel bounding box.\n",
    "The problem of recognizing digits from handwriting is, for instance, important to the postal service when automatically reading zip codes from envelopes.\n",
    "\n",
    "### What You'll Learn\n",
    "We'll show you how to use PyTorch to build a model with two convolutional layers and two fully connected layers to perform the multi-class classification of images provided.\n",
    "In addition, there is a dropout layer after the convolutional layers (and before the first fully connected layer) and another one right after the very first fully connected layer.\n",
    "An approach using the `Sequential` module is available [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
    "\n",
    "The example in the notebook includes both training a model in the notebook and running a [distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html) `PyTorchJob` on the cluster, so you can easily scale up your own models.\n",
    "For the distributed training job you'll need to package the complete trainer code in a Docker image.\n",
    "We'll show you how to do that with Kubeflow Fairing, so that you do not have to leave your favourite notebook environment at all!\n",
    "We'll also include instructions for local development, in case you prefer that.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Kubernetes Nomenclature</b><br>\n",
    "    <code>PyTorchJob</code> is a <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">custom resource (definition) (CRD)</a> provided by the <a href=\"https://www.kubeflow.org/docs/reference/pytorchjob/v1/pytorch/\">PyTorch operator</a>.\n",
    "    <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">Operators</a> extend Kubernetes by capturing domain-specific knowledge on how to deploy and run an application or service, how to deal with failures, and so on.\n",
    "    The lifecycle of a <code>PyTorchJob</code> is managed by the of the PyTorch operator controller.\n",
    "</div>\n",
    "\n",
    "\n",
    "### What You'll Need\n",
    "All you need is this notebook.\n",
    "If you prefer to create your Docker image locally, you must also have a Docker client installed on your machine.\n",
    "\n",
    "## Prerequisites\n",
    "Before we proceed, let's check that we're using the right image, that is, [PyTorch](https://pytorch.org/docs/stable/torch.html) is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-pytorchjob      0.1.3              \n",
      "torch                    1.5.0              \n",
      "torchvision              0.6.0a0+82fd1c8    \n"
     ]
    }
   ],
   "source": [
    "! pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To package the trainer in a container image, we shall need a file (on our cluster) that contains the code as well as a file with the resource definitition of the job for the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_FILE = \"mnist.py\"\n",
    "KUBERNETES_FILE = \"pytorchjob-mnist.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to capture output from a cell with [`%%capture`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture) that usually looks like `some-resource created`.\n",
    "To that end, let's define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from IPython.utils.capture import CapturedIO\n",
    "\n",
    "\n",
    "def get_resource(captured_io: CapturedIO) -> str:\n",
    "    \"\"\"\n",
    "    Gets a resource name from `kubectl apply -f <configuration.yaml>`.\n",
    "\n",
    "    :param str captured_io: Output captured by using `%%capture` cell magic\n",
    "    :return: Name of the Kubernetes resource\n",
    "    :rtype: str\n",
    "    :raises Exception: if the resource could not be created\n",
    "    \"\"\"\n",
    "    out = captured_io.stdout\n",
    "    matches = re.search(r\"^(.+)\\s+created\", out)\n",
    "    if matches is not None:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Cannot get resource as its creation failed: {out}. It may already exist.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Load and Inspect the Data\n",
    "Before we train our model, it is sensible to inspect the data we are about to feed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# See: https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.MNIST\n",
    "mnist = datasets.MNIST(\"datasets\", download=False, train=True, transform=transforms.ToTensor())\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60000, 28, 28]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mnist.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are therefore dealing with 60,000 28x28 pixel greyscale images.\n",
    "These have not yet been scaled into the [0, 1] range, as we can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(255.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.float().min(), mnist.data.float().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "example, example_label = mnist.__getitem__(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(np.squeeze(example))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall normalize the data set (to improve the training speed), which means we need to know the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1307), tensor(0.3081))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.float().mean() / 255, mnist.data.float().std() / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the values we shall hard-code in our transformations within the model.\n",
    "Ideally, we re-compute these based on the training data set to ensure we capture the correct values when the underlying data changes.\n",
    "Our data set is static, though.\n",
    "Note that these values are _always_ re-used (i.e. not re-computed) when predicting (or serving) to minimize training/serving skew.\n",
    "For our demonstration it is fine to define these up front.\n",
    "Batch normalization would be an alternative that scales better with data sets of any size.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>A Note on Batch Normalization</b><br>\n",
    "    <a href=\"https://arxiv.org/abs/1502.03167\">Batch normalization</a> computes the mean and variance per batch of training data and per layer to rescale the batch's input values with the aid of two hyperparameters: β (shift) and γ (scale).\n",
    "    It is typically applied before the activation function (as in the original paper), although there is <a href=\"https://blog.paperspace.com/busting-the-myths-about-batch-normalization/\">no concensus</a> on the matter and there may be valid reasons to apply it afterwards.\n",
    "    Batch normalization allows weights in later layers to be more robust against changes in input values of the earlier ones; while the input of later layers can obviously vary from step to step, the mean and variance will remain fairly constant.\n",
    "    This is because we shuffle the training data set and each batch is therefore on average roughly representative of the entire data set.\n",
    "    Batch normalization limits the distribution of weight values in the later layers of a neural network, and therefore provides a regularizing effect that decreases as the batch size increases.<br><br>\n",
    "    At prediction time, batch normalization requires (fixed) values for the mean and variance to transform the data.\n",
    "    The population statistics are the obvious choice, computed across batches from either moving averages or the exponentially weighted averages.\n",
    "    It is often argued that we use these rather the equivalent values at inference time, because we may not receive batches to predict on; we cannot compute the mean and variance sensibly for individual examples.\n",
    "    While that is certainly true in some cases (e.g. online predictions), the main reason is to avoid training/serving skew.\n",
    "    Even with batches at inference, there may be significant correlation present (e.g. data from the same entity, such as a user, a session, a region, a product, a machine or assembly line, and so on).\n",
    "    In these cases, the mean/variance of the prediction batch may not be representative of the population at large.\n",
    "    Once scaled, these input values may well be near the population mean of zero with unit variance, even though in the overall population they would have been near the tails of the distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear variables as we have no need for these any longer\n",
    "del mnist, example, example_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Before we proceed, we separate the <code>epoch</code> hyperparameter from the main code.\n",
    "    The reason we do that is to ensure we can run the notebook in so-called headless mode with <a href=\"https://papermill.readthedocs.io/en/latest/\">Papermill</a> for custom parameters.\n",
    "    This allows us to test the notebooks end-to-end automatically.\n",
    "    If you check the <a href=\"https://jupyterlab.readthedocs.io/en/stable/user/notebook.html#notebook\">cell tag</a> of the next cell, you can see it is tagged as <code>parameters</code>.\n",
    "    Feel free to ignore it!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Train the Model in the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ultimately want to train the model in a distributed fashion (potentially on GPUs), we put all the code in a single cell.\n",
    "That way we can save the file and include it in a container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "trainer_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_FILE\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Number of processes participating in (distributed) job\n",
    "# See: https://pytorch.org/docs/stable/distributed.html\n",
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "\n",
    "# Custom models must subclass toch.nn.Module and override `forward`\n",
    "# See: https://pytorch.org/docs/stable/nn.html#torch.nn.Module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1\n",
    "\n",
    "\n",
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()\n",
    "\n",
    "\n",
    "def percentage(value):\n",
    "    return \"{: 5.1f}%\".format(100.0 * value)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch} ({percentage(batch_idx / len(train_loader))}) - Loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum batch losses\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    logging.info(\n",
    "        f\"Test accuracy: {correct}/{len(test_loader.dataset)} ({percentage(correct / len(test_loader.dataset))})\"\n",
    "    )\n",
    "\n",
    "    # Log metrics for Katib\n",
    "    logging.info(\"loss={:.4f}\".format(test_loss))\n",
    "    logging.info(\"accuracy={:.4f}\".format(float(correct) / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Training Job\")\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"Batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        metavar=\"N\",\n",
    "        help=\"Batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        metavar=\"N\",\n",
    "        help=\"Number of epochs to train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        metavar=\"LR\",\n",
    "        help=\"Learning rate (default: 1.0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        metavar=\"M\",\n",
    "        help=\"Learning rate's decay rate (default: 0.7)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-cuda\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Disables CUDA (GPU) training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=1, metavar=\"S\", help=\"Random seed (default: 1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"Number of training batches between status log entries\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save-model\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Whether to save the trained model\",\n",
    "    )\n",
    "\n",
    "    if dist.is_available():\n",
    "        parser.add_argument(\n",
    "            \"--backend\",\n",
    "            type=str,\n",
    "            help=\"Distributed backend\",\n",
    "            choices=[dist.Backend.GLOO, dist.Backend.NCCL, dist.Backend.MPI],\n",
    "            default=dist.Backend.GLOO,\n",
    "        )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if should_distribute():\n",
    "        logging.debug(\"Using distributed PyTorch with {} backend\".format(args.backend))\n",
    "        dist.init_process_group(backend=args.backend)\n",
    "\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    train_data = datasets.MNIST(\n",
    "        \"datasets\",\n",
    "        download=False,\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # DistributedSampler partitions the training dataset among the worker processes\n",
    "    train_sampler = (\n",
    "        torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "        if should_distribute()\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"datasets\",\n",
    "            download=False,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if is_distributed():\n",
    "        if use_cuda:\n",
    "            torch.cuda.set_device(torch.cuda.current_device())\n",
    "        model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # See: https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # See: https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_model.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That saves the file as defined by `TRAINER_FILE` but it does not run it.\n",
    "\n",
    "The log entries for 'Katib' are to re-use the same file for [hyperparameter tuning](../../katib/Hyperparameter%20Tuning.ipynb), which is done in a separate notebook.\n",
    "All you need to know for that is that Katib looks for `key=value` entries in the logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>A Note on Activation Functions</b><br>\n",
    "    A common choice for <a href=\"https://arxiv.org/abs/1606.02228\">activation functions</a> is a ReLU (Rectified Linear Unit).\n",
    "    It is linear for non-negative values and zero for negative ones.\n",
    "    The <a href=\"https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\">main benefits of ReLU</a> as opposed to sigmoidal functions (e.g. logistic or `tanh`) are:\n",
    "    <ul>\n",
    "        <li>ReLU and its gradient are very cheap to compute;</li>\n",
    "        <li>Gradients are less likely to vanish, because for (non-)negative values its gradient is constant and therefore does not saturate, which for deep neural networks can <a href=\"https://dl.acm.org/doi/10.1145/3065386\">accelerate convergence</a></li>\n",
    "        <li>ReLU has a regularizing effect, because it promotes <a href=\"https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks\">sparse representations</a> (i.e. some nodes' weights are zero);</li>\n",
    "        <li>Empirically it has been found to work well.</li>\n",
    "    </ul>\n",
    "    ReLU activation functions can cause neurons to 'die' because a large, negative (learned) bias value causes all inputs to be negative, which in turn leads to a zero output.\n",
    "    The neuron has thus become incapable of discriminating different input values.\n",
    "    So-called leaky ReLU activations functions address that issue; these functions are linear but non-zero for negative values, so that their gradients are small but non-zero.\n",
    "    <a href=\"https://arxiv.org/abs/1511.07289\">ELUs</a>, or exponential linear units, are another solution to the problem of dying neurons.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>A Note on CNNs</b><br>\n",
    "    While it is not our intention to cover the basics of <a href=\"https://www.deeplearningbook.org/contents/convnets.html\">convolutional neural networks</a> (CNNs), there are a few matters worth mentioning.\n",
    "    Convolutional layers are spatial feature extractors for images.\n",
    "    A series of convolutional kernels (of the same dimensions) is applied to the image to obtain different versions of the same base image (i.e. filters).\n",
    "    These filters extract patterns hierarchically.\n",
    "    In the first layer, filters typically capture dots, edges, corners, and so on.\n",
    "    With each additional layer, these patterns become more complex and turn from basic geometric shapes into constituents of objects and entire objects.\n",
    "    That is why often the number of filters increases with each additional convolutional layer: to extract more complex patterns.<br><br>\n",
    "    Convolutional layers are often followed by a pooling layer to down-sample the input.\n",
    "    This aids in lowering the computational burden as we increase the number of filters.\n",
    "    A max pooling layer simply picks the largest value of pixels in a small (rectangular) neighbourhood of a single channel (e.g. RGB).\n",
    "    This has the effect of making features <em>locally</em> translation-invariant, which is often desired: whether a feature of interest is on the left or right edge of a pooling window, which is also referred to as a kernel, is largely irrelevant to the problem of image classification.\n",
    "    Note that this may not always be a desired characteristic and depends on the size of the pooling kernel.\n",
    "    For instance, the precice location of tissue damage in living organisms or defects on manufactured products may be very significant indeed.\n",
    "    Pooling kernels are generally chosen to be relatively small compared to the dimensions of the input, which means that local translation invariance is often desired. <br><br>\n",
    "    Another common component of CNNs is a dropout layer.\n",
    "    <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout</a> provides a mechanism for regularization that has proven successful in many applications.\n",
    "    It is suprisingly simple: some nodes' weights (and biases) in a specific layer are set to zero <em>at random</em>, that is, arbitrary nodes are removed from the network during the training step.\n",
    "    This causes the network to not rely on any single node (a.k.a. neuron) for a feature, as each node can be dropped at random.\n",
    "    The network therefore has to learn redundant representations of features.\n",
    "    This is important because of what is referred to as <em>internal covariate shift</em> (often mentioned in connection with <a href=\"http://proceedings.mlr.press/v37/ioffe15.html\">batch normalization</a>): the change of distributions of internal nodes' weights due to all other layers, which can cause nodes to stop learning (i.e. updating their weights).\n",
    "    Thanks to dropout, layers become more robust to changes, although it also means it limits what can be learnt (as always with regularization).\n",
    "    Still, dropout is the neural network's equivalent of the saying you should never put all your eggs in one basket.\n",
    "    Layers with a high risk of overfitting (e.g. layers with many units and lots of inputs) typically have a higher dropout rate.\n",
    "    <br><br>\n",
    "    A nice visual explanation of convolutional layers is available <a href=\"https://cezannec.github.io/Convolutional_Neural_Networks/\">here</a>.\n",
    "    If you're curious what a CNN \"sees\" while training, you can have a look <a href=\"https://poloclub.github.io/cnn-explainer/\">here</a>.\n",
    "</div>\n",
    "\n",
    "Let's see if our code is correct by running it from within our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch: 1 (  0.0%) - Loss: 2.293032646179199\n",
      "INFO:root:Epoch: 1 ( 13.6%) - Loss: 0.5257666110992432\n",
      "INFO:root:Epoch: 1 ( 27.3%) - Loss: 0.08510863780975342\n",
      "INFO:root:Epoch: 1 ( 40.9%) - Loss: 0.32805368304252625\n",
      "INFO:root:Epoch: 1 ( 54.6%) - Loss: 0.3279671370983124\n",
      "INFO:root:Epoch: 1 ( 68.2%) - Loss: 0.06365685909986496\n",
      "INFO:root:Epoch: 1 ( 81.9%) - Loss: 0.29687821865081787\n",
      "INFO:root:Epoch: 1 ( 95.5%) - Loss: 0.03434577211737633\n",
      "INFO:root:Test accuracy: 9834/10000 ( 98.3%)\n",
      "INFO:root:loss=0.0500\n",
      "INFO:root:accuracy=0.9834\n",
      "INFO:root:Epoch: 2 (  0.0%) - Loss: 0.08447802066802979\n",
      "INFO:root:Epoch: 2 ( 13.6%) - Loss: 0.2620002329349518\n",
      "INFO:root:Epoch: 2 ( 27.3%) - Loss: 0.10486980527639389\n",
      "INFO:root:Epoch: 2 ( 40.9%) - Loss: 0.07522107660770416\n",
      "INFO:root:Epoch: 2 ( 54.6%) - Loss: 0.044803790748119354\n",
      "INFO:root:Epoch: 2 ( 68.2%) - Loss: 0.06450511515140533\n",
      "INFO:root:Epoch: 2 ( 81.9%) - Loss: 0.25487586855888367\n",
      "INFO:root:Epoch: 2 ( 95.5%) - Loss: 0.01875779777765274\n",
      "INFO:root:Test accuracy: 9859/10000 ( 98.6%)\n",
      "INFO:root:loss=0.0399\n",
      "INFO:root:accuracy=0.9859\n",
      "INFO:root:Epoch: 3 (  0.0%) - Loss: 0.029139619320631027\n",
      "INFO:root:Epoch: 3 ( 13.6%) - Loss: 0.09397225826978683\n",
      "INFO:root:Epoch: 3 ( 27.3%) - Loss: 0.11303514242172241\n",
      "INFO:root:Epoch: 3 ( 40.9%) - Loss: 0.14118748903274536\n",
      "INFO:root:Epoch: 3 ( 54.6%) - Loss: 0.05904180556535721\n",
      "INFO:root:Epoch: 3 ( 68.2%) - Loss: 0.04524335265159607\n",
      "INFO:root:Epoch: 3 ( 81.9%) - Loss: 0.27801263332366943\n",
      "INFO:root:Epoch: 3 ( 95.5%) - Loss: 0.03176506236195564\n",
      "INFO:root:Test accuracy: 9886/10000 ( 98.9%)\n",
      "INFO:root:loss=0.0359\n",
      "INFO:root:accuracy=0.9886\n",
      "INFO:root:Epoch: 4 (  0.0%) - Loss: 0.07127423584461212\n",
      "INFO:root:Epoch: 4 ( 13.6%) - Loss: 0.20250867307186127\n",
      "INFO:root:Epoch: 4 ( 27.3%) - Loss: 0.0050563933327794075\n",
      "INFO:root:Epoch: 4 ( 40.9%) - Loss: 0.14717304706573486\n",
      "INFO:root:Epoch: 4 ( 54.6%) - Loss: 0.10025180876255035\n",
      "INFO:root:Epoch: 4 ( 68.2%) - Loss: 0.13863351941108704\n",
      "INFO:root:Epoch: 4 ( 81.9%) - Loss: 0.10420405864715576\n",
      "INFO:root:Epoch: 4 ( 95.5%) - Loss: 0.004818277433514595\n",
      "INFO:root:Test accuracy: 9887/10000 ( 98.9%)\n",
      "INFO:root:loss=0.0329\n",
      "INFO:root:accuracy=0.9887\n",
      "INFO:root:Epoch: 5 (  0.0%) - Loss: 0.008954059332609177\n",
      "INFO:root:Epoch: 5 ( 13.6%) - Loss: 0.19676166772842407\n",
      "INFO:root:Epoch: 5 ( 27.3%) - Loss: 0.0015074732946231961\n",
      "INFO:root:Epoch: 5 ( 40.9%) - Loss: 0.09220609813928604\n",
      "INFO:root:Epoch: 5 ( 54.6%) - Loss: 0.015971817076206207\n",
      "INFO:root:Epoch: 5 ( 68.2%) - Loss: 0.05801410600543022\n",
      "INFO:root:Epoch: 5 ( 81.9%) - Loss: 0.07174661010503769\n",
      "INFO:root:Epoch: 5 ( 95.5%) - Loss: 0.0020152931101620197\n",
      "INFO:root:Test accuracy: 9909/10000 ( 99.1%)\n",
      "INFO:root:loss=0.0306\n",
      "INFO:root:accuracy=0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run $TRAINER_FILE --epochs $epochs --log-interval 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "trainer_dockerfile"
    ]
   },
   "source": [
    "This trains the model in the notebook, but does not distribute it across nodes (a.k.a. pods) in our cluster.\n",
    "To that end, we have to first create a Docker image with the code, push it to a registry (e.g. [Docker Hub](https://hub.docker.com/), [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/), [ECR](https://aws.amazon.com/ecr/), [GCR](https://cloud.google.com/container-registry/), or similar), and then define the Kubernetes resource that uses the image.\n",
    "\n",
    "## How to Create a Docker Image with Kubeflow Fairing\n",
    "Kubeflow Fairing is a Python SDK that allows you to build, push, and optionally run containerized ML models without leaving Jupyter!\n",
    "To build and push Docker images from within a notebook, please check out the [Kubeflow Fairing notebook](../../fairing/Kubeflow%20Fairing.ipynb).\n",
    "All you need is the `TRAINER_FILE` and access to a container registry.\n",
    "\n",
    "## How to Create a Docker Image Manually\n",
    "If you are comfortable with Docker (or prefer to use it as a part of your CI/CD setup), you can create a Dockerfile as follows.\n",
    "You do have to download the `TRAINER_FILE` contents to your local machine.\n",
    "The Kubernetes cluster does not have a Docker daemon available to build your image, so you must do it locally.\n",
    "It uses [containerd](https://containerd.io/) to run workloads (only) instead.\n",
    "\n",
    "The Dockerfile looks as follows:\n",
    "\n",
    "```\n",
    "FROM mesosphere/kubeflow:1.0.1-0.5.0-pytorch-1.5.0-gpu\n",
    "ADD mnist.py /\n",
    "ADD datasets /datasets\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/mnist.py\"]\n",
    "```\n",
    "\n",
    "If GPU support is not needed, you can leave off the `-gpu` suffix from the image.\n",
    "`mnist.py` is the trainer code you have to download to your local machine.\n",
    "\n",
    "Then it's easy to push images to your container registry:\n",
    "\n",
    "```bash\n",
    "docker build -t <docker_image_name_with_tag> .\n",
    "docker push <docker_image_name_with_tag>\n",
    "```\n",
    "\n",
    "The image is available as `mesosphere/kubeflow:mnist-pytorch-1.0.1-0.5.0` in case you want to skip it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Create a Distributed `PyTorchJob`\n",
    "For large training jobs, we wish to run our trainer in a distributed mode.\n",
    "Once the notebook server cluster can access the Docker image from the registry, we can launch a distributed PyTorch job.\n",
    "\n",
    "The specification for a distributed `PyTorchJob` is defined using YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytorchjob-mnist.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $KUBERNETES_FILE\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: \"PyTorchJob\"\n",
    "metadata:\n",
    "  name: \"pytorchjob-mnist\"\n",
    "spec:\n",
    "  pytorchReplicaSpecs:\n",
    "    Master:\n",
    "      replicas: 1\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "            - name: pytorch\n",
    "              # modify this property if you would like to use a custom image\n",
    "              image: mesosphere/kubeflow:mnist-pytorch-1.0.1-0.5.0\n",
    "              # TODO: Add arguments as required!\n",
    "              args:\n",
    "                - --epochs\n",
    "                - \"15\"\n",
    "                - --seed\n",
    "                - \"7\"\n",
    "                - --log-interval\n",
    "                - \"256\"\n",
    "              # Comment out these resources when using only CPUs\n",
    "              resources:\n",
    "                limits:\n",
    "                  nvidia.com/gpu: 1\n",
    "    Worker:\n",
    "      replicas: 2\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          containers:\n",
    "            - name: pytorch\n",
    "              # modify this property if you like to use a custom image\n",
    "              image: mesosphere/kubeflow:mnist-pytorch-1.0.1-0.5.0\n",
    "              args:\n",
    "                # TODO: Add arguments as required!\n",
    "                - --epochs\n",
    "                - \"15\"\n",
    "                - --seed\n",
    "                - \"7\"\n",
    "                - --log-interval\n",
    "                - \"256\"\n",
    "              # Comment out these resources when using only CPUs\n",
    "              resources:\n",
    "                limits:\n",
    "                  nvidia.com/gpu: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this does is create one master and two worker pods.\n",
    "These can be adjusted via `spec.pytorchReplicaSpecs.<type>.replicas` with `<type>` either `Master` or `Worker`.\n",
    "The distributed sampler passes chunks of the training data set equally to the pods.\n",
    "\n",
    "Custom training arguments can be passed to the container by means of the `spec.containers.args`.\n",
    "What is supported is visible in `main()` of `mnist.py`.\n",
    "\n",
    "The container image specified (twice) is what is for the code shown above.\n",
    "Still, it's best to change the image name listed under the comments of the specification to use an equivalent image in your own container registry, to ensure everythng works as expected.\n",
    "\n",
    "The job can run in parallel on CPUs or GPUs, provided these are available in your cluster.\n",
    "To switch to CPUs or define resource limites, please adjust `spec.containers.resources` as required.\n",
    "\n",
    "You can either execute the following commands on your local machine with `kubectl` or directly from the notebook.\n",
    "If you do run these locally, you cannot rely on cell magic, so you have to manually copy-paste the variables' values wherever you see `$SOME_VARIABLE`.\n",
    "If you execute the following commands on your own machine (and not inside the notebook), you obviously do not need the bang `!` either.\n",
    "In that case, you have to set the user namespace for all subsequent commands:\n",
    "\n",
    "```bash\n",
    "kubectl config set-context --current --namespace=<insert-namespace>\n",
    "```\n",
    "\n",
    "Please change the namespace to whatever has been set up by your administrator.\n",
    "\n",
    "Let's deploy the distributed training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture pytorch_output --no-stderr\n",
    "! kubectl create -f $KUBERNETES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_JOB = get_resource(pytorch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl describe $PYTORCH_JOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output roughly looks like this:\n",
    "\n",
    "```yaml\n",
    "Name:         pytorchjob-mnist\n",
    "...\n",
    "Kind:         PyTorchJob\n",
    "...\n",
    "Events:\n",
    "  Type    Reason                   Age   From              Message\n",
    "  ----    ------                   ----  ----              -------\n",
    "  Normal  SuccessfulCreatePod      8s    pytorch-operator  Created pod: pytorchjob-mnist-master-0\n",
    "  Normal  SuccessfulCreateService  8s    pytorch-operator  Created service: pytorchjob-mnist-master-0\n",
    "  Normal  SuccessfulCreatePod      8s    pytorch-operator  Created pod: pytorchjob-mnist-worker-0\n",
    "  Normal  SuccessfulCreatePod      8s    pytorch-operator  Created pod: pytorchjob-mnist-worker-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the pods created, matching the specified number of replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         READY   STATUS    RESTARTS   AGE\n",
      "pytorchjob-mnist-master-0   1/1     Running   0          34s\n",
      "pytorchjob-mnist-worker-0   1/1     Running   0          34s\n",
      "pytorchjob-mnist-worker-1   1/1     Running   0          34s\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l job-name=pytorchjob-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job name matches `metadata.name` from the YAML.\n",
    "\n",
    "As per our specification, the training runs for 15 epochs.\n",
    "During that time, we can stream the logs from the `Master` pod to follow the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using distributed PyTorch with gloo backend\n",
      "Epoch: 1 (  0.0%) - Loss: 2.3082287311553955\n",
      "Epoch: 1 ( 81.8%) - Loss: 0.04400685429573059\n",
      "Test accuracy: 9814/10000 ( 98.1%)\n",
      "loss=0.0568\n",
      "accuracy=0.9814\n",
      "Epoch: 2 (  0.0%) - Loss: 0.12475904077291489\n",
      "Epoch: 2 ( 81.8%) - Loss: 0.022593408823013306\n",
      "Test accuracy: 9859/10000 ( 98.6%)\n",
      "loss=0.0382\n",
      "accuracy=0.9859\n",
      "Epoch: 3 (  0.0%) - Loss: 0.10856910794973373\n",
      "Epoch: 3 ( 81.8%) - Loss: 0.020849494263529778\n",
      "Test accuracy: 9876/10000 ( 98.8%)\n",
      "loss=0.0332\n",
      "accuracy=0.9876\n",
      "Epoch: 4 (  0.0%) - Loss: 0.09520195424556732\n",
      "Epoch: 4 ( 81.8%) - Loss: 0.02882443554699421\n",
      "Test accuracy: 9893/10000 ( 98.9%)\n",
      "loss=0.0314\n",
      "accuracy=0.9893\n",
      "Epoch: 5 (  0.0%) - Loss: 0.09747796505689621\n",
      "Epoch: 5 ( 81.8%) - Loss: 0.019643055275082588\n",
      "Test accuracy: 9899/10000 ( 99.0%)\n",
      "loss=0.0305\n",
      "accuracy=0.9899\n",
      "Epoch: 6 (  0.0%) - Loss: 0.02446436882019043\n",
      "Epoch: 6 ( 81.8%) - Loss: 0.005831033922731876\n",
      "Test accuracy: 9909/10000 ( 99.1%)\n",
      "loss=0.0286\n",
      "accuracy=0.9909\n",
      "Epoch: 7 (  0.0%) - Loss: 0.024502133950591087\n",
      "Epoch: 7 ( 81.8%) - Loss: 0.01055021770298481\n",
      "Test accuracy: 9909/10000 ( 99.1%)\n",
      "loss=0.0276\n",
      "accuracy=0.9909\n",
      "Epoch: 8 (  0.0%) - Loss: 0.02548063173890114\n",
      "Epoch: 8 ( 81.8%) - Loss: 0.0008087620371952653\n",
      "Test accuracy: 9907/10000 ( 99.1%)\n",
      "loss=0.0272\n",
      "accuracy=0.9907\n",
      "Epoch: 9 (  0.0%) - Loss: 0.022815870121121407\n",
      "Epoch: 9 ( 81.8%) - Loss: 0.003437922103330493\n",
      "Test accuracy: 9908/10000 ( 99.1%)\n",
      "loss=0.0272\n",
      "accuracy=0.9908\n",
      "Epoch: 10 (  0.0%) - Loss: 0.04595055431127548\n",
      "Epoch: 10 ( 81.8%) - Loss: 0.00236672000028193\n",
      "Test accuracy: 9908/10000 ( 99.1%)\n",
      "loss=0.0267\n",
      "accuracy=0.9908\n",
      "Epoch: 11 (  0.0%) - Loss: 0.040099453181028366\n",
      "Epoch: 11 ( 81.8%) - Loss: 0.003305426798760891\n",
      "Test accuracy: 9912/10000 ( 99.1%)\n",
      "loss=0.0267\n",
      "accuracy=0.9912\n",
      "Epoch: 12 (  0.0%) - Loss: 0.03731080889701843\n",
      "Epoch: 12 ( 81.8%) - Loss: 0.006072529591619968\n",
      "Test accuracy: 9909/10000 ( 99.1%)\n",
      "loss=0.0270\n",
      "accuracy=0.9909\n",
      "Epoch: 13 (  0.0%) - Loss: 0.006752971094101667\n",
      "Epoch: 13 ( 81.8%) - Loss: 0.0009820580016821623\n",
      "Test accuracy: 9912/10000 ( 99.1%)\n",
      "loss=0.0269\n",
      "accuracy=0.9912\n",
      "Epoch: 14 (  0.0%) - Loss: 0.019512642174959183\n",
      "Epoch: 14 ( 81.8%) - Loss: 0.0012978784507140517\n",
      "Test accuracy: 9912/10000 ( 99.1%)\n",
      "loss=0.0268\n",
      "accuracy=0.9912\n",
      "Epoch: 15 (  0.0%) - Loss: 0.011409440077841282\n",
      "Epoch: 15 ( 81.8%) - Loss: 0.0017151175998151302\n",
      "Test accuracy: 9911/10000 ( 99.1%)\n",
      "loss=0.0267\n",
      "accuracy=0.9911\n"
     ]
    }
   ],
   "source": [
    "! kubectl logs -f pytorchjob-mnist-master-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it may take a while when the image has to be pulled from the registry.\n",
    "Once the status for all pods is 'Running', it usually takes a few minutes, depending on the arguments and resources of the cluster.\n",
    "\n",
    "To delete the job, just execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete $PYTORCH_JOB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
